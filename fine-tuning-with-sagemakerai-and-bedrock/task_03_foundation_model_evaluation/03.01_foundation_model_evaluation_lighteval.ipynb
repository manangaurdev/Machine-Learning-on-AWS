{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91a4f64-2568-4b90-8fb2-17f5ccde1c4d",
   "metadata": {},
   "source": [
    "# Comparing Model Performance after Fine-Tuning\n",
    "In this example, we will take the pre-existing SageMaker endpoints that you deployed in previous exercises and use them to generate data that can be leveraged for quality comparison. This data can be used to take a quantitative approach to judge the efficacy of fine-tuning your models.\n",
    "\n",
    "This example will run through samples of the Samsum dataset (paper here) on the Hugging Face data hub to generate summaries of earnings calls transcripts and use the [lighteval](https://huggingface.co/docs/lighteval/index) from Hugging Face for analysis on those summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353628ef-4cf9-4957-85ee-1667ac4de611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages and restart the kernel\n",
    "%pip install datasets pandas matplotlib numpy boto3 tqdm lighteval[math]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c20c83d-2050-494c-acd6-0c9f575eb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "\n",
    "# Import LightEval metrics\n",
    "from lighteval.metrics.metrics_sample import ROUGE, Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1341fb-a37e-4f9f-9d3f-32233d58427f",
   "metadata": {},
   "source": [
    "#### Update the base model and fine-tuned model endpoints with the names of the endpoints you previously created. \n",
    "You can find these in **SageMaker Studio > Deployments > Endpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821e1176-f2af-4e7f-9273-48b2d67e22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SageMaker client\n",
    "sm_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Configure the SageMaker endpoint names\n",
    "BASE_MODEL_ENDPOINT = \"DeepSeek-R1-Distill-Llama-8B-endpoint\"  # Update with Base model endpoint name\n",
    "FINETUNED_MODEL_ENDPOINT = \"DeepSeek-R1-Distill-Llama-8B-finetuned\"  # Update with Fine-tuned model endpoint name\n",
    "\n",
    "# Define the model to evaluate\n",
    "model_to_evaluate = {\n",
    "    \"name\": \"Fine-tuned DeepSeek-R1-Distill-Llama-8B\", \n",
    "    \"endpoint\": FINETUNED_MODEL_ENDPOINT\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de5205-0203-441b-b8d6-e2a8ef6c7fae",
   "metadata": {},
   "source": [
    "Here you will use the the Samsum dataset. The dataset is pre-split into training and test data. We will limit the number of samples to evaluate for the fine-tuned and base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89eec7-cac2-4295-a0d6-495bb445d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to use for evaluation - using SAMSum\n",
    "dataset_name = \"samsum\"\n",
    "split = \"test\"\n",
    "\n",
    "# Limit the number of samples to evaluate (for faster execution)\n",
    "num_samples = 10\n",
    "\n",
    "# Load the test split of the SAMSum dataset\n",
    "dataset = load_dataset(dataset_name, split=split)\n",
    "dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "print(f\"Loaded SAMSum dataset with {len(dataset)} samples\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "sample = dataset[0]\n",
    "print(\"Dialogue:\\n\", sample[\"dialogue\"], \"\\n\")\n",
    "print(\"Summary:\\n\", sample[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a5e2c-39e9-4d51-a394-b666ffde44f2",
   "metadata": {},
   "source": [
    "#### Next, we will create functions to interact with the SageMaker endpoints, define metrics we want to calculate (ROUGE), and define how to evaluate the models with the Samsum dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957d8b1e-0761-4f00-ae6d-dc0c9530e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows you to interact with a deployed SageMaker endpoint to get predictions from the DeepSeek model\n",
    "def invoke_sagemaker_endpoint(payload, endpoint_name):\n",
    "    \"\"\"\n",
    "    Invoke a SageMaker endpoint with the given payload.\n",
    "    \n",
    "    Args:\n",
    "        payload (dict): The input data to send to the endpoint\n",
    "        endpoint_name (str): The name of the SageMaker endpoint\n",
    "        \n",
    "    Returns:\n",
    "        dict: The response from the endpoint\n",
    "    \"\"\"\n",
    "    response = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    response_body = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8cd7d2a-ed91-45fa-8f1c-dfe42f9ebc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LightEval metrics calculators\n",
    "rouge_metrics = ROUGE(\n",
    "    methods=[\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    multiple_golds=False,\n",
    "    bootstrap=False,\n",
    "    normalize_gold=None,\n",
    "    normalize_pred=None\n",
    ")\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate all evaluation metrics for summarization using LightEval.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of generated summaries\n",
    "        references (list): List of reference summaries\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all metric scores\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Create Doc objects for the Rouge and BertScore metrics\n",
    "    docs = []\n",
    "    for reference in references:\n",
    "        docs.append(Doc(\n",
    "            {\"target\": reference},\n",
    "            choices=[reference],  # Dummy choices\n",
    "            gold_index=0  # Dummy gold_index\n",
    "        ))\n",
    "    \n",
    "    # Calculate ROUGE scores for each prediction-reference pair\n",
    "    rouge_scores = {'rouge1_f': [], 'rouge2_f': [], 'rougeL_f': []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # For ROUGE calculation\n",
    "        rouge_result = rouge_metrics.compute(golds=[ref], predictions=[pred])\n",
    "        rouge_scores['rouge1_f'].append(rouge_result['rouge1'])\n",
    "        rouge_scores['rouge2_f'].append(rouge_result['rouge2'])\n",
    "        rouge_scores['rougeL_f'].append(rouge_result['rougeL'])\n",
    "    \n",
    "    # Average ROUGE scores\n",
    "    for key in rouge_scores:\n",
    "        metrics[key] = sum(rouge_scores[key]) / len(rouge_scores[key])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccac55f4-463b-4a25-bbbd-529f76fbc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries_with_model(endpoint_name, dataset):\n",
    "    \"\"\"\n",
    "    Generate summaries using a model deployed on SageMaker.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): SageMaker endpoint name\n",
    "        dataset: Dataset containing dialogues\n",
    "        \n",
    "    Returns:\n",
    "        list: Generated summaries\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Generating summaries\"):\n",
    "        dialogue = example[\"dialogue\"]\n",
    "        \n",
    "        # Prepare the prompt for the model\n",
    "        prompt = f\"\"\"Please summarize the following conversation concisely:\n",
    "\n",
    "Conversation:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        \n",
    "        # Payload for SageMaker endpoint\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Call the model endpoint\n",
    "        try:\n",
    "            response = invoke_sagemaker_endpoint(payload, endpoint_name)\n",
    "            \n",
    "            # Extract the generated text\n",
    "            if isinstance(response, list):\n",
    "                prediction = response[0].get('generated_text', '').strip()\n",
    "            elif isinstance(response, dict):\n",
    "                prediction = response.get('generated_text', '').strip()\n",
    "            else:\n",
    "                prediction = str(response).strip\n",
    "            \n",
    "            # Clean up the generated text\n",
    "            if \"Summary:\" in prediction:\n",
    "                prediction = prediction.split(\"Summary:\", 1)[1].strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "            prediction = \"Error generating summary.\"\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a872572-5d5d-4a0e-98f6-e656f26ba29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_samsum(model_config, dataset):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned model on the SamSum dataset using both automated and human metrics.\n",
    "    \n",
    "    Args:\n",
    "        model_config (dict): Model configuration with name and endpoint\n",
    "        dataset: SamSum dataset for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    model_name = model_config[\"name\"]\n",
    "    endpoint_name = model_config[\"endpoint\"]\n",
    "    \n",
    "    print(f\"\\nEvaluating model: {model_name} on endpoint: {endpoint_name}\")\n",
    "    \n",
    "    # Get references\n",
    "    references = [example[\"summary\"] for example in dataset]\n",
    "    \n",
    "    # Generate summaries\n",
    "    print(\"\\nGenerating summaries...\")\n",
    "    predictions = generate_summaries_with_model(endpoint_name, dataset)\n",
    "    \n",
    "    # Calculate automated metrics using LightEval\n",
    "    print(\"\\nCalculating evaluation metrics with LightEval...\")\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    # Format results\n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"num_samples\": len(dataset),\n",
    "        \"metrics\": metrics,\n",
    "        \"predictions\": predictions[:5],  # First 5 predictions\n",
    "        \"references\": references[:5]     # First 5 references\n",
    "    }\n",
    "    \n",
    "    # Print key results\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"ROUGE-1 F1: {metrics['rouge1_f']:.4f}\")\n",
    "    print(f\"ROUGE-2 F1: {metrics['rouge2_f']:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {metrics['rougeL_f']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080aa020-0aaf-438d-b0cb-dd503d248feb",
   "metadata": {},
   "source": [
    "#### In this section, we evaluate the performance of both our base model (DeepSeek-R1-Distill-Llama-8B) and our fine-tuned model on the SAMSum dataset using ROUGE metrics, which are standard for evaluating text summarization quality.\n",
    "\n",
    "The evaluation process:\n",
    "\n",
    "We first evaluate the base model against the SAMSum test set to establish a baseline performance. Then, we evaluate our fine-tuned model on the same dataset to measure improvements. Both evaluations calculate ROUGE-1, ROUGE-2, and ROUGE-L scores, which respectively measure:\n",
    "\n",
    "ROUGE-1: Unigram overlap between generated and reference summaries\n",
    "ROUGE-2: Bigram overlap (captures more fluency and coherence)\n",
    "ROUGE-L: Longest common subsequence (measures sentence structure similarity)\n",
    "\n",
    "The results are saved to JSON files for later analysis and comparison. These metrics will help us quantify how much our fine-tuning process has improved the model's summarization capabilities compared to the original base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db13a7-81a3-4279-bde5-afbc75ca2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the base and fine-tuned models using LightEval metrics\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate both models for comparison\n",
    "base_model_config = {\n",
    "    \"name\": \"Base DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"endpoint\": BASE_MODEL_ENDPOINT\n",
    "}\n",
    "\n",
    "# Evaluate base model\n",
    "base_model_results = evaluate_model_on_samsum(base_model_config, dataset)\n",
    "base_model_results[\"evaluation_time\"] = time.time() - start_time\n",
    "\n",
    "# Start timing fine-tuned model\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "finetuned_model_results = evaluate_model_on_samsum(model_to_evaluate, dataset)\n",
    "finetuned_model_results[\"evaluation_time\"] = time.time() - start_time\n",
    "\n",
    "# Save results\n",
    "base_file_name = base_model_config[\"name\"].replace(' ', '_').lower()\n",
    "finetuned_file_name = model_to_evaluate[\"name\"].replace(' ', '_').lower()\n",
    "\n",
    "with open(f\"{base_file_name}_results.json\", \"w\") as f:\n",
    "    json.dump(base_model_results, f)\n",
    "    \n",
    "with open(f\"{finetuned_file_name}_results.json\", \"w\") as f:\n",
    "    json.dump(finetuned_model_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4944d-ad64-448d-91aa-e7222d7ebe8a",
   "metadata": {},
   "source": [
    "Create a tablular view to compare the base model and fine-tuned model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336396df-7175-4069-9794-b425501aee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "# Add base model metrics\n",
    "comparison_data.append({\n",
    "    \"Model\": base_model_config[\"name\"],\n",
    "    \"ROUGE-1 F1\": base_model_results[\"metrics\"][\"rouge1_f\"],\n",
    "    \"ROUGE-2 F1\": base_model_results[\"metrics\"][\"rouge2_f\"],\n",
    "    \"ROUGE-L F1\": base_model_results[\"metrics\"][\"rougeL_f\"],\n",
    "    \"Evaluation Time (s)\": base_model_results[\"evaluation_time\"]\n",
    "})\n",
    "\n",
    "# Add fine-tuned model metrics\n",
    "comparison_data.append({\n",
    "    \"Model\": model_to_evaluate[\"name\"],\n",
    "    \"ROUGE-1 F1\": finetuned_model_results[\"metrics\"][\"rouge1_f\"],\n",
    "    \"ROUGE-2 F1\": finetuned_model_results[\"metrics\"][\"rouge2_f\"],\n",
    "    \"ROUGE-L F1\": finetuned_model_results[\"metrics\"][\"rougeL_f\"],\n",
    "    \"Evaluation Time (s)\": finetuned_model_results[\"evaluation_time\"]\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07ee65-270a-41ce-bdff-620318c673f6",
   "metadata": {},
   "source": [
    "Show a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fa953-173a-433b-ac83-7f75a45907eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROUGE and BERTScore metrics for both models\n",
    "metrics_to_plot = [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"]\n",
    "models = comparison_df[\"Model\"].tolist()\n",
    "\n",
    "# Create a grouped bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(metrics_to_plot))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    values = [comparison_df.loc[i, metric] for metric in metrics_to_plot]\n",
    "    plt.bar(index + i*bar_width, values, bar_width, label=model)\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Summarization Performance Comparison')\n",
    "plt.xticks(index + bar_width/2, metrics_to_plot)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabf665-2e81-4b17-8ce2-3b7aec06e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement from base to fine-tuned model\n",
    "improvement_data = {}\n",
    "\n",
    "for metric in [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"]:\n",
    "    base_value = comparison_df.loc[0, metric]\n",
    "    finetuned_value = comparison_df.loc[1, metric]\n",
    "    \n",
    "    if not pd.isna(base_value) and not pd.isna(finetuned_value):\n",
    "        abs_improvement = finetuned_value - base_value\n",
    "        pct_improvement = (abs_improvement / base_value) * 100 if base_value > 0 else float('inf')\n",
    "        \n",
    "        improvement_data[metric] = {\n",
    "            \"Base Model\": base_value,\n",
    "            \"Fine-tuned Model\": finetuned_value,\n",
    "            \"Absolute Improvement\": abs_improvement,\n",
    "            \"% Improvement\": pct_improvement\n",
    "        }\n",
    "\n",
    "# Create DataFrame for improvement metrics\n",
    "improvement_df = pd.DataFrame({\n",
    "    \"Metric\": list(improvement_data.keys()),\n",
    "    \"Base Score\": [improvement_data[m][\"Base Model\"] for m in improvement_data],\n",
    "    \"Fine-tuned Score\": [improvement_data[m][\"Fine-tuned Model\"] for m in improvement_data],\n",
    "    \"Absolute Improvement\": [improvement_data[m][\"Absolute Improvement\"] for m in improvement_data],\n",
    "    \"% Improvement\": [f\"{improvement_data[m]['% Improvement']:.2f}%\" for m in improvement_data]\n",
    "})\n",
    "\n",
    "print(\"Improvement Analysis:\")\n",
    "improvement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fd3c6-19a9-41c7-97e2-c03ab2023ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example predictions from both models\n",
    "num_examples = min(3, len(dataset))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Dialogue (truncated): {dataset[i]['dialogue'][:150]}...\")\n",
    "    print(f\"\\nReference Summary: {dataset[i]['summary']}\")\n",
    "    \n",
    "    # Show predictions from both models\n",
    "    print(f\"\\nBase Model Summary: {base_model_results['predictions'][i]}\")\n",
    "    print(f\"\\nFine-tuned Model Summary: {finetuned_model_results['predictions'][i]}\")\n",
    "    \n",
    "    # Calculate ROUGE scores for this example using LightEval\n",
    "    base_rouge = rouge_metrics.compute(golds=[dataset[i]['summary']], predictions=[base_model_results['predictions'][i]])\n",
    "    finetuned_rouge = rouge_metrics.compute(golds=[dataset[i]['summary']], predictions=[finetuned_model_results['predictions'][i]])\n",
    "    \n",
    "    print(\"\\nROUGE Scores (LightEval):\")\n",
    "    print(f\"Base Model - ROUGE-1: {base_rouge['rouge1']:.4f}, ROUGE-2: {base_rouge['rouge2']:.4f}, ROUGE-L: {base_rouge['rougeL']:.4f}\")\n",
    "    print(f\"Fine-tuned - ROUGE-1: {finetuned_rouge['rouge1']:.4f}, ROUGE-2: {finetuned_rouge['rouge2']:.4f}, ROUGE-L: {finetuned_rouge['rougeL']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
