{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bfca55-c771-4e26-a13b-3451f6bef06a",
   "metadata": {},
   "source": [
    "# Applying Bedrock Guardrails to the DeepSeek R1 Distill Llama 8B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41c9b6-e40e-4408-8209-faa9db60da8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "Guardrails can be used to implement safeguards for your generative AI applications that are customized to your use cases and aligned with your responsible AI policies. Guardrails allows you to:\n",
    "\n",
    "- Configure denied topics\n",
    "- Filter harmful content\n",
    "- Remove sensitive information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fa5fb-589a-4d83-8083-ba926f327b4e",
   "metadata": {},
   "source": [
    "## The`ApplyGuardrail` API allows you to assess any text using pre-configured Bedrock Guardrails, without invoking the foundation models.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Content Validation**: Send any text input or output to the ApplyGuardrail API to have it evaluated against your defined topic avoidance rules, content filters, PII detectors, and word blocklists. You can evaluate user inputs and FM generated outputs independently.\n",
    "\n",
    "2. **Flexible Deployment**: Integrate the Guardrails API anywhere in your application flow to validate data before processing or serving results to users. E.g. For a RAG application, you can now evaluate the user input prior to performing the retrieval instead of waiting until the final response generation.\n",
    "\n",
    "3. **Decoupled from Foundation Models**: ApplyGuardrail is decoupled from foundational models. You can now use Guardrails without invoking Foundation Models.\n",
    "\n",
    "You can use the assessment results to design the experience on your generative AI application. Let's now walk through a code-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd569723-3c66-4569-82a0-a2538eb73921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Start by installing the dependencies to ensure we have a recent version\n",
    "!pip install --upgrade --force-reinstall boto3\n",
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa341fb6-dce9-4b79-a070-a98020870221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Dict, Any\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "client = boto3.client('bedrock', region_name=sess.boto_region_name)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=sess.boto_region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3480e72-04ac-4caf-86bd-0fae2cb8fcc1",
   "metadata": {},
   "source": [
    "### Important: Create a Guardrail First\n",
    "\n",
    "Before running the code to apply a guardrail, you need to create a guardrail in Amazon Bedrock. We will create a guardrail that blocks input prompts and output responses from the model providing fiduciary advice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d82166-6725-4dab-96fa-f0ab2b1a0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_guardrail(\n",
    "    name='fiduciary-advice-llm-guardrail',\n",
    "    description='Prevents the our model from providing fiduciary advice.',\n",
    "    topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {\n",
    "                'name': 'Fiduciary Advice',\n",
    "                'definition': 'Providing personalized advice or recommendations on managing financial assets, investments, or trusts in a fiduciary capacity or assuming related obligations and liabilities.',\n",
    "                'examples': [\n",
    "                    'What stocks should I invest in for my retirement?',\n",
    "                    'Is it a good idea to put my money in a mutual fund?',\n",
    "                    'How should I allocate my 401(k) investments?',\n",
    "                    'What type of trust fund should I set up for my children?',\n",
    "                    'Should I hire a financial advisor to manage my investments?'\n",
    "                ],\n",
    "                'type': 'DENY'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    contentPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {\n",
    "                'type': 'SEXUAL',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'VIOLENCE',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'HATE',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'INSULTS',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'MISCONDUCT',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'PROMPT_ATTACK',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'NONE'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    wordPolicyConfig={\n",
    "        'wordsConfig': [\n",
    "            {'text': 'fiduciary advice'},\n",
    "            {'text': 'investment recommendations'},\n",
    "            {'text': 'stock picks'},\n",
    "            {'text': 'financial planning guidance'},\n",
    "            {'text': 'portfolio allocation advice'},\n",
    "            {'text': 'retirement fund suggestions'},\n",
    "            {'text': 'wealth management tips'},\n",
    "            {'text': 'trust fund setup'},\n",
    "            {'text': 'investment strategy'},\n",
    "            {'text': 'financial advisor recommendations'}\n",
    "        ],\n",
    "        'managedWordListsConfig': [\n",
    "            {'type': 'PROFANITY'}\n",
    "        ]\n",
    "    },\n",
    "    sensitiveInformationPolicyConfig={\n",
    "        'piiEntitiesConfig': [\n",
    "            {'type': 'EMAIL', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'PHONE', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'NAME', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'US_SOCIAL_SECURITY_NUMBER', 'action': 'BLOCK'},\n",
    "            {'type': 'US_BANK_ACCOUNT_NUMBER', 'action': 'BLOCK'},\n",
    "            {'type': 'CREDIT_DEBIT_CARD_NUMBER', 'action': 'BLOCK'}\n",
    "        ],\n",
    "        'regexesConfig': [\n",
    "            {\n",
    "                'name': 'Account Number',\n",
    "                'description': 'Matches account numbers in the format XXXXXX1234',\n",
    "                'pattern': r'\\b\\d{6}\\d{4}\\b',\n",
    "                'action': 'ANONYMIZE'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    contextualGroundingPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {\n",
    "                'type': 'GROUNDING',\n",
    "                'threshold': 0.75\n",
    "            },\n",
    "            {\n",
    "                'type': 'RELEVANCE',\n",
    "                'threshold': 0.75\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    blockedInputMessaging=\"\"\"I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \"\"\",\n",
    "    blockedOutputsMessaging=\"\"\"I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \"\"\",\n",
    "    tags=[\n",
    "        {'key': 'purpose', 'value': 'fiduciary-advice-prevention'},\n",
    "        {'key': 'environment', 'value': 'production'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a845db-851d-405d-b6f5-819a214ffa52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "guardrail_id = response['guardrailId']\n",
    "guardrail_version = response['version'] \n",
    "\n",
    "print(f\"Guardrail ID: {guardrail_id}\")\n",
    "print(f\"Guardrail Version: {guardrail_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98f4b1-129f-4c1a-8ce9-15482a52c52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of Input Prompt being Analyzed\n",
    "content = [\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Is the AB503 Product a better investment than the S&P 500?\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Here's an example of something that should pass\n",
    "\n",
    "#content = [\n",
    "    #{\n",
    "    #    \"text\": {\n",
    "   #         \"text\": \"What is the rate you offer for the AB503 Product?\"\n",
    "  #      }\n",
    " #   }\n",
    "#]\n",
    "\n",
    "# Call the ApplyGuardrail API\n",
    "try:\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='INPUT',  # or 'INPUT' depending on your use case\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    print(\"API Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    # Check the action taken by the guardrail\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        print(\"\\nGuardrail intervened. Output:\")\n",
    "        for output in response['outputs']:\n",
    "            print(output['text'])\n",
    "    else:\n",
    "        print(\"\\nGuardrail did not intervene.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"\\nAPI Response (if available):\")\n",
    "    try:\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except NameError:\n",
    "        print(\"No response available due to early exception.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189e5ab-4a38-417e-b766-e84efbea17b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# An Example of Analyzing an Output Response, This time using Contexual Grounding\n",
    "\n",
    "content = [\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%\",\n",
    "            \"qualifiers\": [\"grounding_source\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Whats the Guaranteed return rate of your AB503 Product\",\n",
    "            \"qualifiers\": [\"query\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Our Guaranteed Rate is 7%\",\n",
    "            \"qualifiers\": [\"guard_content\"],\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Call the ApplyGuardrail API\n",
    "try:\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='OUTPUT',  # or 'INPUT' depending on your use case\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    print(\"API Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    # Check the action taken by the guardrail\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        print(\"\\nGuardrail intervened. Output:\")\n",
    "        for output in response['outputs']:\n",
    "            print(output['text'])\n",
    "    else:\n",
    "        print(\"\\nGuardrail did not intervene.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"\\nAPI Response (if available):\")\n",
    "    try:\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except NameError:\n",
    "        print(\"No response available due to early exception.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225dd51-c1a3-4621-a902-f9700d8ac6b4",
   "metadata": {},
   "source": [
    "## Using ApplyGuardrail API with a Third-Party or Self-Hosted Model\n",
    "\n",
    "A common use case for the ApplyGuardrail API is in conjunction with a Language Model from a non Amazon Bedrock provider, or a model that you self-host. This combination allows you to apply guardrails to the input or output of any request.\n",
    "\n",
    "The general flow would be:\n",
    "1. Receive an input for your Model\n",
    "2. Apply the guardrail to this input using the ApplyGuardrail API\n",
    "3. If the input passes the guardrail, send it to your Model for Inference\n",
    "4. Receive the output from your Model\n",
    "5. Apply the Guardrail to your output\n",
    "6. Return the final (potentially modified) output\n",
    "\n",
    "### Here's a diagram illustrating this process:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/applyguardrail.png\" alt=\"ApplyGuardrail API Flow\" style=\"max-width: 100%;\">\n",
    "</div>\n",
    "\n",
    "Let's walk through this with a code example that demonstrates this process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00340f-253e-451b-840e-ebd77165d740",
   "metadata": {},
   "source": [
    "### For our examples today we will use a Self-Hosted SageMaker Model, but this could be any third-party model as well\n",
    "\n",
    "We will use the `DeepSeek-R1-Distill-Llama-8B` model that we deployed earlier on a SageMaker Endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f2797-24ec-4361-bd09-20d31efb5509",
   "metadata": {},
   "source": [
    "### Incorporating the ApplyGuardrail API into our Self-Hosted Model\n",
    "\n",
    "---\n",
    "We've created a `TextGenerationWithGuardrails` class that integrates the ApplyGuardrail API with our SageMaker endpoint to ensure protected text generation. This class includes the following key methods:\n",
    "\n",
    "1. `generate_text`: Calls our Language Model via a SageMaker endpoint to generate text based on the input.\n",
    "\n",
    "2. `analyze_text`: A core method that applies our guardrail using the ApplyGuardrail API. It int|erprets the API response to determine if the guardrail passed or intervened.\n",
    "\n",
    "3. `analyze_prompt` and `analyze_output`: These methods use `analyze_text` to apply our guardrail to the input prompt and generated output, respectively. They return a tuple indicating whether the guardrail passed and any associated message.\n",
    "\n",
    "The class looks to implement the diagram above. It works as follows:\n",
    "\n",
    "1. It first checks the input prompt using `analyze_prompt`.\n",
    "2. If the input passes the guardrail, it generates text using `generate_text`.\n",
    "3. The generated text is then checked using `analyze_output`.\n",
    "4. If both guardrails pass, the generated text is returned. Otherwise, an intervention message is provided.\n",
    "\n",
    "This structure allows for comprehensive safety checks both before and after text generation, with clear handling of cases where guardrails intervene. It's designed to easily integrate with larger applications while providing flexibility for error handling and customization based on guardrail results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d96fdd1-8eff-4351-8063-4bd8e5883c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "class TextGenerationWithGuardrails:\n",
    "    def __init__(self, endpoint_name: str, model_id: str, guardrail_id: str, guardrail_version: str, sagemaker_session=None):\n",
    "        \"\"\"\n",
    "        Initialize the text generation class with guardrails.\n",
    "        \n",
    "        Args:\n",
    "            endpoint_name: The SageMaker endpoint name\n",
    "            model_id: The model ID (optional but useful for documentation)\n",
    "            guardrail_id: The AWS Bedrock guardrail ID\n",
    "            guardrail_version: The AWS Bedrock guardrail version\n",
    "            sagemaker_session: SageMaker session object\n",
    "        \"\"\"\n",
    "        # Create predictor directly instead of using retrieve_default\n",
    "        self.predictor = Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            serializer=JSONSerializer(),\n",
    "            deserializer=JSONDeserializer()\n",
    "        )\n",
    "        self.model_id = model_id\n",
    "        self.bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "        self.guardrail_id = guardrail_id\n",
    "        self.guardrail_version = guardrail_version\n",
    "\n",
    "    def generate_text(self, inputs: str, max_new_tokens: int = 256, temperature: float = 0.0) -> str:\n",
    "        \"\"\"Generate text using the specified SageMaker endpoint.\"\"\"\n",
    "        payload = {\n",
    "            \"inputs\": inputs,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stop\": \"<|eot_id|>\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        response = self.predictor.predict(payload)\n",
    "        return response.get('generated_text', '')\n",
    "\n",
    "    def analyze_text(self, grounding_source: str, query: str, guard_content: str, source: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Analyze text using the ApplyGuardrail API with contextual grounding.\n",
    "        Returns a tuple (passed, message, details) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - details is a dictionary containing the full API response for further analysis if needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = [\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": grounding_source,\n",
    "                        \"qualifiers\": [\"grounding_source\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": query,\n",
    "                        \"qualifiers\": [\"query\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": guard_content,\n",
    "                        \"qualifiers\": [\"guard_content\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = self.bedrock_runtime.apply_guardrail(\n",
    "                guardrailIdentifier=self.guardrail_id,\n",
    "                guardrailVersion=self.guardrail_version,\n",
    "                source=source,\n",
    "                content=content\n",
    "            )\n",
    "            \n",
    "            action = response.get(\"action\", \"\")\n",
    "            if action == \"NONE\":\n",
    "                return True, \"\", response\n",
    "            elif action == \"GUARDRAIL_INTERVENED\":\n",
    "                message = response.get(\"outputs\", [{}])[0].get(\"text\", \"Guardrail intervened\")\n",
    "                return False, message, response\n",
    "            else:\n",
    "                return False, f\"Unknown action: {action}\", response\n",
    "        except ClientError as e:\n",
    "            print(f\"Error applying guardrail: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_prompt(self, grounding_source: str, query: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the input prompt.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, query, \"INPUT\")\n",
    "\n",
    "    def analyze_output(self, grounding_source: str, query: str, generated_text: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the generated output.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, generated_text, \"OUTPUT\")\n",
    "\n",
    "    def generate_and_analyze(self, grounding_source: str, query: str, max_new_tokens: int = 256, temperature: float = 0.0) -> Tuple[bool, str, str]:\n",
    "        \"\"\"\n",
    "        Generate text and analyze it with guardrails.\n",
    "        Returns a tuple (passed, message, generated_text) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - generated_text is the text generated by the model (if guardrail passed) or an empty string.\n",
    "        \"\"\"\n",
    "        # First, analyze the prompt\n",
    "        prompt_passed, prompt_message, _ = self.analyze_prompt(grounding_source, query)\n",
    "        if not prompt_passed:\n",
    "            return False, prompt_message, \"\"\n",
    "\n",
    "        # If prompt passes, generate text\n",
    "        generated_text = self.generate_text(query, max_new_tokens, temperature)\n",
    "\n",
    "        # Analyze the generated text\n",
    "        output_passed, output_message, _ = self.analyze_output(grounding_source, query, generated_text)\n",
    "        if not output_passed:\n",
    "            return False, output_message, \"\"\n",
    "\n",
    "        return True, \"\", generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ac6a0-293b-4ea4-b314-0aead2a5af75",
   "metadata": {},
   "source": [
    "### Now let's see a Sample Usage in action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecc620-17db-40ea-8536-f96b06ad6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import boto3\n",
    "    import json\n",
    "    import datetime\n",
    "    from sagemaker.session import Session\n",
    "    from botocore.exceptions import ClientError\n",
    "\n",
    "    # Function to list available SageMaker endpoints\n",
    "    def list_sagemaker_endpoints():\n",
    "        try:\n",
    "            sagemaker_client = boto3.client('sagemaker')\n",
    "            response = sagemaker_client.list_endpoints()\n",
    "            return response.get('Endpoints', [])\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing endpoints: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Get available SageMaker endpoints\n",
    "    print(\"Fetching available SageMaker endpoints...\")\n",
    "    endpoints = list_sagemaker_endpoints()\n",
    "    \n",
    "    if not endpoints:\n",
    "        print(\"No SageMaker endpoints found. Please create an endpoint first.\")\n",
    "        endpoint_name = input(\"Please enter your endpoint name manually: \")\n",
    "        if not endpoint_name:\n",
    "            print(\"No endpoint name provided. Exiting.\")\n",
    "            return\n",
    "    else:\n",
    "        # Display available endpoints\n",
    "        print(\"\\nAvailable SageMaker Endpoints:\")\n",
    "        for idx, endpoint in enumerate(endpoints):\n",
    "            name = endpoint.get('EndpointName', 'Unknown')\n",
    "            status = endpoint.get('EndpointStatus', 'Unknown')\n",
    "            created_time = endpoint.get('CreationTime', 'Unknown')\n",
    "            \n",
    "            # Find endpoints containing \"deepseek\" in the name (likely to be deepseek model endpoints)\n",
    "            is_deepseek = \"deepseek\" in name.lower()\n",
    "            highlight = \" (Likely Deepseek model)\" if is_deepseek else \"\"\n",
    "            \n",
    "            print(f\"{idx+1}. {name} - Status: {status}, Created: {created_time}{highlight}\")\n",
    "        \n",
    "        # Try to automatically select a DeepSeek endpoint\n",
    "        deepseek_endpoints = [ep for ep in endpoints if \"deepseek\" in ep.get('EndpointName', '').lower()]\n",
    "        \n",
    "        if deepseek_endpoints:\n",
    "            # Sort by creation time (newest first) if available\n",
    "            sorted_endpoints = sorted(\n",
    "                deepseek_endpoints, \n",
    "                key=lambda x: x.get('CreationTime', datetime.datetime.min.replace(tzinfo=datetime.timezone.utc)),\n",
    "                reverse=True\n",
    "            )\n",
    "            selected_endpoint = sorted_endpoints[0]\n",
    "            endpoint_name = selected_endpoint.get('EndpointName')\n",
    "            print(f\"\\nAutomatically selected Deepseek endpoint: {endpoint_name}\")\n",
    "        else:\n",
    "            # Let user choose if no Deepseek endpoint found\n",
    "            choice = input(f\"\\nEnter endpoint number (1-{len(endpoints)}) or name: \")\n",
    "            \n",
    "            try:\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(endpoints):\n",
    "                    endpoint_name = endpoints[idx].get('EndpointName')\n",
    "                else:\n",
    "                    print(\"Invalid choice, using the first endpoint.\")\n",
    "                    endpoint_name = endpoints[0].get('EndpointName')\n",
    "            except ValueError:\n",
    "                # If input is not a number, use it as the endpoint name\n",
    "                endpoint_name = choice\n",
    "\n",
    "    # Model ID for DeepSeek R1 Distill\n",
    "    model_id = \"deepseek-llm-r1-distill-llama-8b\"\n",
    "    \n",
    "    # Create Bedrock client to access guardrails\n",
    "    try:\n",
    "        bedrock_client = boto3.client('bedrock')\n",
    "        \n",
    "        # List all available guardrails\n",
    "        print(\"Fetching available guardrails...\")\n",
    "        guardrails_response = bedrock_client.list_guardrails()\n",
    "        \n",
    "        if 'guardrails' not in guardrails_response or not guardrails_response['guardrails']:\n",
    "            print(\"No guardrails found. Please create a guardrail in AWS Bedrock first.\")\n",
    "            return\n",
    "\n",
    "        # Display available guardrails\n",
    "        print(\"\\nAvailable Guardrails:\")\n",
    "        for idx, guardrail in enumerate(guardrails_response['guardrails']):\n",
    "            guardrail_id = guardrail.get('id')\n",
    "            name = guardrail.get('name', 'Unnamed')\n",
    "            print(f\"{idx+1}. {name} (ID: {guardrail_id})\")\n",
    "        \n",
    "        # Use the first guardrail in the list\n",
    "        selected_guardrail = guardrails_response['guardrails'][0]\n",
    "        guardrail_id = selected_guardrail['id']\n",
    "        guardrail_name = selected_guardrail.get('name', 'Unnamed')\n",
    "        \n",
    "        print(f\"\\nSelected guardrail: {guardrail_name} (ID: {guardrail_id})\")\n",
    "        \n",
    "        # Get detailed information about the guardrail\n",
    "        guardrail_info = bedrock_client.get_guardrail(\n",
    "            guardrailIdentifier=guardrail_id,\n",
    "            guardrailVersion='DRAFT'\n",
    "        )\n",
    "        \n",
    "        print(f\"Using guardrail version: DRAFT (Created: {guardrail_info.get('createdAt')})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing guardrails: {e}\")\n",
    "        print(\"\\nFalling back to manual guardrail ID entry...\")\n",
    "        \n",
    "        # Prompt for guardrail ID if API access fails\n",
    "        guardrail_id = input(\"Please enter your guardrail ID: \")\n",
    "        if not guardrail_id:\n",
    "            print(\"No guardrail ID provided. Exiting.\")\n",
    "            return\n",
    "    \n",
    "    guardrail_version = \"DRAFT\"  # Using DRAFT version as shown in your example\n",
    "    \n",
    "    # Create SageMaker session\n",
    "    boto_session = boto3.Session()\n",
    "    sagemaker_session = Session(boto_session)\n",
    "    \n",
    "    # Query and grounding information\n",
    "    query = \"What are is the Guarenteed Rate of Return for AB503 Product\"\n",
    "    grounding_source = \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%\"\n",
    "    max_new_tokens = 512\n",
    "    temperature = 0.0\n",
    "\n",
    "    # Initialize TextGenerationWithGuardrails\n",
    "    text_gen = TextGenerationWithGuardrails(\n",
    "        endpoint_name=endpoint_name,\n",
    "        model_id=model_id,\n",
    "        guardrail_id=guardrail_id,\n",
    "        guardrail_version=guardrail_version,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    # Bold text function\n",
    "    def bold(text):\n",
    "        return f\"\\033[1m{text}\\033[0m\"\n",
    "    \n",
    "    # Analyze input\n",
    "    print(bold(\"\\n=== Input Analysis ===\\n\"))\n",
    "    input_passed, input_message, input_details = text_gen.analyze_prompt(grounding_source, query)\n",
    "    if not input_passed:\n",
    "        print(f\"Input Guardrail Intervened. The response to the User is: {input_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(input_details, indent=2))\n",
    "        print()\n",
    "        return\n",
    "    else:\n",
    "        print(\"Input Prompt Passed The Guardrail Check - Moving to Generate the Response\\n\")\n",
    "\n",
    "    # Generate text\n",
    "    print(bold(\"\\n=== Text Generation ===\\n\"))\n",
    "    generated_text = text_gen.generate_text(query, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    print(f\"Here is what the Model Responded with: {generated_text}\\n\")\n",
    "\n",
    "    # Analyze output\n",
    "    print(bold(\"\\n=== Output Analysis ===\\n\"))\n",
    "    print(\"Analyzing Model Response with the Response Guardrail\\n\")\n",
    "    output_passed, output_message, output_details = text_gen.analyze_output(grounding_source, query, generated_text)\n",
    "    if not output_passed:\n",
    "        print(f\"Output Guardrail Intervened. The response to the User is: {output_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(output_details, indent=2))\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Model Response Passed. The information presented to the user is: {generated_text}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f33392-61ab-4fc5-9279-a62fbdc62a12",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61498705-28fa-485d-9371-244c9353b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_guardrail(guardrailIdentifier=\"<replace with guardrail ID or ARN>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
